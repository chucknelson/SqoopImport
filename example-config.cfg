### Import job config
# REFERENCE ONLY

### Database Source
# Connection
dbConnectionString=

# Credentials
hadoopCredentialPath=
dbUserName=
dbCredentialName=

# Database
dbServerName=
dbName=
dbSchemaName=

# Table(s) to Import
# Table Name (if single table import)
tableName=

# Table List (if multiple table import)
tableListFileName=

### Sqoop Import Options
# Import Type: Supports "job" | "query" | "table"
importType=

# Mappers, will default to 1 if no value set
numMappers=
# Split-by column required if using "query" import type with multiple mappers
splitByColumn= 

### Import Type: Job
jobName=

### Import Type: Query
# Query string can be defined to fit the RDBMS you're targeting (e.g., Netezza, Oracle, MSSQL, MSSQL Linked Servers, etc.)
# Should always include \$CONDITIONS in the WHERE clause for Sqoop use
# Use \$TABLE if you need to reference the active table name for the import (i.e., tableName or names in table list)
queryString=

# Destination Type: Support "hdfs" | "hive"
destinationType=

# Staging Directory
# HDFS location that will be used to stage imported data before copying to the final destination
# Helps to protect against deleting data before a Sqoop job successfully finishes
stagingDir=

# Desination Directory
# If "job" type, will override the destination defined in the job - required for non-incremental jobs
# If "hdfs" type, this is the final destination for the imported data
# If "hive" type, this is just a staging area before import into the hive database location
# NOTE: As of 12/23/2016, a Hive DB cannot be stored on the same blob container as this directory
destinationDir=

# Set to "true" to remove old data before import
# This is required for non-incremental jobs/imports
destinationDirOverwrite=

# Destination Type: Hive
# Destination Hive DB needs to exist
destinationHiveDB=