### Import job config

### Database Source
# Connection
dbConnectionString="jdbc:[connector]://"

# Credentials
hadoopCredentialPath="jceks://file$HOME/.hadoop-credential/credential.jceks"
dbUserName=
dbCredentialName=

# Database
dbServerName= # Name or IP, usually only used for query reference (e.g., MSSQL Linked Server)
dbName=
dbSchemaName=

# Table(s) to Import
# Table Name (if single table import)
tableName=

# Table List (if multiple table import)
tableListFileName=

### Sqoop Import Options
importType="table" # Supports "table" and "query"

# Mappers
numMappers=
splitByColumn= # Required if using query import type with multiple mappers

# Import Type - Query
# Query string can be defined to fit the RDBMS you're targeting (e.g., Netezza, Oracle, MSSQL, MSSQL Linked Servers, etc.)
# Should always include \$CONDITIONS in the WHERE clause for Sqoop use
# Use \$TABLE if you need to reference the active table name for the import (i.e., tableName or names in table list)
queryString="SELECT * FROM [${dbServerName}].[${dbName}].[${dbSchemaName}].[\$TABLE] WHERE \$CONDITIONS"

# Import Destination
destinationType="hive" # Supports "hive" and "hdfs"

# Destination Type - Hive
# Destination Hive DB needs to exist
destinationHiveDB=

# Desination Directory
# If "hive" type, this is just a staging area
# If "hdfs" type, this is the final destination for the imported data
destinationDir=